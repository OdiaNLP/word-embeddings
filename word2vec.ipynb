{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from indicnlp.tokenize.indic_tokenize import trivial_tokenize_indic\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# set up the logging to monitor gensim\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def tokenize_text(text: List[str]) -> List[List[str]]:\n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    return [trivial_tokenize_indic(sent) for sent in tqdm(text, desc='tokenize', unit=' sentences')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def train_w2v(tokenized_text: List[List[str]], size: int = 100, window: int = 5, min_count: int = 1, epochs: int = 10,\n",
    "              random_seed: int = 123, vec_file_path: Union[str, None] = None, ):\n",
    "    \"Learn word2vec embeddings\"\n",
    "    # count the number of cores\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    # create word2vec model\n",
    "    model = Word2Vec(\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=cores - 1,\n",
    "        seed=random_seed,\n",
    "    )\n",
    "    # build vocab\n",
    "    model.build_vocab(sentences=tokenized_text, progress_per=1000000)  # show progress after processing every 1M words\n",
    "    # train\n",
    "    model.train(sentences=tokenized_text, total_examples=model.corpus_count, epochs=epochs,\n",
    "                report_delay=10)  # show progress after every 10 seconds\n",
    "    if vec_file_path is not None:\n",
    "        model.wv.save_word2vec_format(vec_file_path, binary=False)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "\n",
    "For learning the Odia word embeddings, we need monolingual Odia text data.\n",
    "You can possibly scrape data from an online source such as Wikipedia.\n",
    "For our experiments now, let's take the Odia monolingual text data available as part of the [Indic NLP corpus](https://github.com/AI4Bharat/indicnlp_corpus)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "filename = os.path.join('data/or')\n",
    "assert os.path.isfile(filename)  # sanity check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read lines from file: 100%|██████████| 3594672/3594672 [00:02<00:00, 1523841.53it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    lines = [s.strip() for s in tqdm(f.readlines(), desc='read lines from file')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize: 100%|██████████| 3594672/3594672 [01:21<00:00, 44039.58 sentences/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokens = tokenize_text(lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute frequencies of tokens: 100%|██████████| 3594672/3594672 [00:21<00:00, 169660.67 sentences/s]\n"
     ]
    }
   ],
   "source": [
    "num_running_toks, num_unique_toks = 0, 0\n",
    "counter = Counter()\n",
    "for toks in tqdm(tokens, desc='compute frequencies of tokens', unit=' sentences'):\n",
    "    counter.update(toks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3,594,672\n",
      "Number of unique words or equivalantly, the size of vocabulary: 778,862\n",
      "Number of running words: 51,151,273\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of sentences: {len(lines):,}')\n",
    "print(f'Number of unique words or equivalantly, the size of vocabulary: {len(counter):,}')\n",
    "print(f'Number of running words: {sum([freq for _, freq in counter.items()]):,}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[('।', 3393061),\n (',', 1191253),\n ('ଓ', 534792),\n ('ଏହି', 437185),\n ('ପାଇଁ', 373726),\n ('ସେ', 240775),\n ('ବୋଲି', 239837),\n ('ପରେ', 224959),\n ('କରି', 221628),\n ('ଏକ', 213516),\n ('ମଧ୍ୟ', 210907),\n ('ଏବଂ', 198988),\n ('କରିଥିଲେ', 195168),\n ('ସହ', 177040),\n ('-', 174796),\n ('ଖବର', 169373),\n ('.', 166728),\n ('କରିବା', 166276),\n ('ନେଇ', 161728),\n ('ବେଳେ', 156327)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common words\n",
    "counter.most_common(n=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learn embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:35:17: collecting all words and their counts\n",
      "INFO - 02:35:17: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 02:35:21: PROGRESS: at sentence #1000000, processed 14406915 words, keeping 356423 word types\n",
      "INFO - 02:35:24: PROGRESS: at sentence #2000000, processed 28227997 words, keeping 518060 word types\n",
      "INFO - 02:35:28: PROGRESS: at sentence #3000000, processed 42532970 words, keeping 692676 word types\n",
      "INFO - 02:35:30: collected 778862 word types from a corpus of 51151273 raw words and 3594672 sentences\n",
      "INFO - 02:35:30: Loading a fresh vocabulary\n",
      "INFO - 02:35:30: effective_min_count=20 retains 72827 unique words (9% of original 778862, drops 706035)\n",
      "INFO - 02:35:30: effective_min_count=20 leaves 49262024 word corpus (96% of original 51151273, drops 1889249)\n",
      "INFO - 02:35:31: deleting the raw counts dictionary of 778862 items\n",
      "INFO - 02:35:31: sample=0.001 downsamples 22 most-common words\n",
      "INFO - 02:35:31: downsampling leaves estimated 43948889 word corpus (89.2% of prior 49262024)\n",
      "INFO - 02:35:31: estimated required memory for 72827 words and 100 dimensions: 94675100 bytes\n",
      "INFO - 02:35:31: resetting layer weights\n",
      "INFO - 02:35:46: training model with 3 workers on 72827 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 02:35:47: EPOCH 1 - PROGRESS: at 1.55% examples, 718123 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:35:57: EPOCH 1 - PROGRESS: at 17.28% examples, 704262 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:36:07: EPOCH 1 - PROGRESS: at 35.23% examples, 745695 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 02:36:17: EPOCH 1 - PROGRESS: at 54.47% examples, 770214 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:36:27: EPOCH 1 - PROGRESS: at 72.69% examples, 753063 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:36:37: EPOCH 1 - PROGRESS: at 88.26% examples, 760557 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:36:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:36:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:36:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:36:44: EPOCH - 1 : training on 51151273 raw words (43948895 effective words) took 58.2s, 754767 effective words/s\n",
      "INFO - 02:36:45: EPOCH 2 - PROGRESS: at 1.66% examples, 777457 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:36:55: EPOCH 2 - PROGRESS: at 18.55% examples, 757767 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:37:05: EPOCH 2 - PROGRESS: at 37.87% examples, 801941 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:37:15: EPOCH 2 - PROGRESS: at 55.04% examples, 777041 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 02:37:25: EPOCH 2 - PROGRESS: at 73.92% examples, 764890 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:37:35: EPOCH 2 - PROGRESS: at 90.57% examples, 781777 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:37:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:37:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:37:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:37:41: EPOCH - 2 : training on 51151273 raw words (43948356 effective words) took 56.4s, 779882 effective words/s\n",
      "INFO - 02:37:42: EPOCH 3 - PROGRESS: at 1.62% examples, 757226 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:37:52: EPOCH 3 - PROGRESS: at 20.86% examples, 850322 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:38:02: EPOCH 3 - PROGRESS: at 40.08% examples, 848148 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:38:12: EPOCH 3 - PROGRESS: at 60.47% examples, 845345 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:38:22: EPOCH 3 - PROGRESS: at 78.99% examples, 834388 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:38:32: EPOCH 3 - PROGRESS: at 95.68% examples, 826142 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:38:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:38:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:38:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:38:34: EPOCH - 3 : training on 51151273 raw words (43950616 effective words) took 53.2s, 826462 effective words/s\n",
      "INFO - 02:38:35: EPOCH 4 - PROGRESS: at 1.71% examples, 806058 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:38:45: EPOCH 4 - PROGRESS: at 19.02% examples, 776988 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:38:55: EPOCH 4 - PROGRESS: at 38.35% examples, 812272 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:39:05: EPOCH 4 - PROGRESS: at 56.88% examples, 801183 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:39:15: EPOCH 4 - PROGRESS: at 75.86% examples, 791948 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:39:25: EPOCH 4 - PROGRESS: at 89.48% examples, 772755 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:39:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:39:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:39:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:39:31: EPOCH - 4 : training on 51151273 raw words (43951926 effective words) took 57.3s, 767042 effective words/s\n",
      "INFO - 02:39:32: EPOCH 5 - PROGRESS: at 1.73% examples, 811916 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:39:42: EPOCH 5 - PROGRESS: at 20.51% examples, 836518 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:39:52: EPOCH 5 - PROGRESS: at 38.76% examples, 820861 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:40:02: EPOCH 5 - PROGRESS: at 59.60% examples, 834743 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:40:12: EPOCH 5 - PROGRESS: at 79.44% examples, 840308 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:40:22: EPOCH 5 - PROGRESS: at 96.93% examples, 836791 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:40:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:40:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:40:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:40:24: EPOCH - 5 : training on 51151273 raw words (43949639 effective words) took 52.5s, 837446 effective words/s\n",
      "INFO - 02:40:25: EPOCH 6 - PROGRESS: at 1.81% examples, 844352 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:40:35: EPOCH 6 - PROGRESS: at 20.62% examples, 840724 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:40:45: EPOCH 6 - PROGRESS: at 34.90% examples, 740059 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:40:55: EPOCH 6 - PROGRESS: at 51.84% examples, 738690 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:41:05: EPOCH 6 - PROGRESS: at 70.40% examples, 731967 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:41:15: EPOCH 6 - PROGRESS: at 85.60% examples, 735364 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:41:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:41:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:41:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:41:22: EPOCH - 6 : training on 51151273 raw words (43950066 effective words) took 58.7s, 749310 effective words/s\n",
      "INFO - 02:41:23: EPOCH 7 - PROGRESS: at 1.86% examples, 863013 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:41:33: EPOCH 7 - PROGRESS: at 20.88% examples, 849964 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:41:43: EPOCH 7 - PROGRESS: at 38.47% examples, 813946 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:41:53: EPOCH 7 - PROGRESS: at 57.83% examples, 812470 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:42:03: EPOCH 7 - PROGRESS: at 77.56% examples, 814731 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:42:13: EPOCH 7 - PROGRESS: at 93.39% examples, 806238 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:42:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:42:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:42:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:42:17: EPOCH - 7 : training on 51151273 raw words (43945643 effective words) took 54.7s, 804105 effective words/s\n",
      "INFO - 02:42:18: EPOCH 8 - PROGRESS: at 1.79% examples, 834486 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:42:28: EPOCH 8 - PROGRESS: at 19.41% examples, 792250 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:42:38: EPOCH 8 - PROGRESS: at 38.97% examples, 825485 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 02:42:48: EPOCH 8 - PROGRESS: at 57.99% examples, 814829 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:42:58: EPOCH 8 - PROGRESS: at 76.28% examples, 798078 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:43:08: EPOCH 8 - PROGRESS: at 93.27% examples, 805692 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:43:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:43:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:43:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:43:12: EPOCH - 8 : training on 51151273 raw words (43948777 effective words) took 54.7s, 803739 effective words/s\n",
      "INFO - 02:43:13: EPOCH 9 - PROGRESS: at 1.75% examples, 820794 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:43:23: EPOCH 9 - PROGRESS: at 21.02% examples, 856675 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:43:33: EPOCH 9 - PROGRESS: at 40.63% examples, 859889 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:43:43: EPOCH 9 - PROGRESS: at 61.41% examples, 857133 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:43:53: EPOCH 9 - PROGRESS: at 80.59% examples, 856223 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:03: EPOCH 9 - PROGRESS: at 99.07% examples, 853928 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:03: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:44:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:44:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:44:03: EPOCH - 9 : training on 51151273 raw words (43948162 effective words) took 51.5s, 853941 effective words/s\n",
      "INFO - 02:44:04: EPOCH 10 - PROGRESS: at 1.86% examples, 864346 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:14: EPOCH 10 - PROGRESS: at 21.27% examples, 865719 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:24: EPOCH 10 - PROGRESS: at 40.73% examples, 861110 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:34: EPOCH 10 - PROGRESS: at 59.76% examples, 836148 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:44: EPOCH 10 - PROGRESS: at 77.00% examples, 806430 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:54: EPOCH 10 - PROGRESS: at 92.14% examples, 794854 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 02:44:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:44:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:44:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:44:59: EPOCH - 10 : training on 51151273 raw words (43949155 effective words) took 55.6s, 790312 effective words/s\n",
      "INFO - 02:44:59: training on a 511512730 raw words (439491235 effective words) took 552.7s, 795205 effective words/s\n",
      "INFO - 02:44:59: storing 72827x100 projection weights into embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "w2vmodel = train_w2v(tokenized_text=tokens, size=100, window=5, min_count=20, epochs=10, random_seed=123,\n",
    "                     vec_file_path=os.path.join('embeddings.txt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate embeddings\n",
    "\n",
    "Here we evaluate the embeddings learned by just  👀  at the neighbours of a few words and examining if they are similar."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[('ଗଛକୁ', 0.8128724694252014),\n ('ଗଛଗୁଡ଼ିକ', 0.7983400821685791),\n ('ଆମ୍ବଗଛ', 0.7657703161239624),\n ('ଗଛର', 0.7544435262680054),\n ('ତାଳଗଛ', 0.753383994102478),\n ('ଗଛଟିଏ', 0.7315728664398193),\n ('ଗଛଟି', 0.7306054830551147),\n ('ଗଛପତ୍ର', 0.7241652011871338),\n ('ବୃକ୍ଷ', 0.714855968952179),\n ('ଗଛଗୁଡିକୁ', 0.7127572298049927)]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find words similar to a given word\n",
    "w2vmodel.wv.most_similar('ଗଛ', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[('ସଂଗୀତ', 0.9450031518936157),\n ('ସଂଙ୍ଗୀତ', 0.8198200464248657),\n ('ଓଡ଼ିଶୀ', 0.7925612926483154),\n ('ସଙ୍ଗିତ', 0.7761118412017822),\n ('ନାଟ୍ୟ', 0.7644525766372681),\n ('ନୃତ୍ୟ', 0.7506800889968872),\n ('ସଙ୍ଗୀତରେ', 0.7364827394485474),\n ('ସଙ୍ଗୀତର', 0.7345959544181824),\n ('ହିନ୍ଦୁସ୍ଥାନୀ', 0.7241092920303345),\n ('ନୃତ୍ୟାଙ୍ଗନା', 0.71225905418396)]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar('ସଙ୍ଗୀତ', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[('ଚଳଚିତ୍ର', 0.8641869425773621),\n ('ଫିଲ୍ମ', 0.8291294574737549),\n ('ସିନେମା', 0.8122552633285522),\n ('ସିନେ', 0.748349666595459),\n ('ଚଳଚ୍ଚିତ୍ରର', 0.7161926031112671),\n ('ଧାରାବାହିକ', 0.7104039192199707),\n ('ଚଳଚ୍ଚିତ୍ରଟି', 0.7052581906318665),\n ('ସିନେମାର', 0.7023676037788391),\n ('ଧାରାବାହିକର', 0.6961969137191772),\n ('ଆଲବମ୍', 0.6868106722831726)]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar('ଚଳଚ୍ଚିତ୍ର', topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}